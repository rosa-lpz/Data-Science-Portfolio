


import pandas as pd
import matplotlib.pyplot as plt
import os

!pip install plotly
import plotly.graph_objects as go
from plotly.subplots import make_subplots


%run "variables.py"


%run "functions.ipynb"








files_names = [
    ("PdM_telemetry.csv", "df_telemetry"),
    ("PdM_errors.csv", "df_errors"),
    ("PdM_machines.csv", "df_machines"),
    ("PdM_maint.csv", "df_maint"),
    ("PdM_failures.csv", "df_failures")
]


dataframes=getDataFrames(files_names=files_names,data_dir=DATA_DIR)
print(dataframes)





df_telemetry= dataframes["df_telemetry"]
df_errors= dataframes["df_errors"]
df_machines= dataframes["df_machines"]
df_maint= dataframes["df_maint"]
df_failures= dataframes["df_failures"]





showDataFrame(files_names=files_names)





#files = ["PdM_telemetry.csv", "PdM_errors.csv", "PdM_machines.csv", "PdM_maint.csv","PdM_failures.csv"]
#df_names= ["df_telemetry","df_errors", "df_machines", "df_maint", "df_failures"] 





# Function to explore a list of dataframes
exploreDataFrames(dataframe_list=dataframes,dataframe_names=df_names)








analyzeDFsFeatures(dataframes_list=dataframes,dataframe_names=df_names)





import pandas as pd
import numpy as np

# Function to handle missing values
def handle_missing_values(df, df_name):
    print(f"Handling Missing Values for {df_name}")
    for col in df.columns:
        if df[col].isnull().any():
            if pd.api.types.is_numeric_dtype(df[col]):
                df[col].fillna(df[col].median(), inplace=True)
                print(f"Imputed missing values in '{col}' using median.")
            else:
                df[col].fillna(df[col].mode()[0], inplace=True)
                print(f"Imputed missing values in '{col}' using mode.")

# Function to address outliers using winsorizing
def address_outliers(df, df_name):
    print(f"Addressing Outliers for {df_name}")
    if df_name == 'df_telemetry':
      for col in ['volt', 'rotate', 'pressure', 'vibration']:
          df[col] = np.clip(df[col], df[col].quantile(0.01), df[col].quantile(0.99))
          print(f"Winsorized '{col}' at 1st and 99th percentiles.")

# Function to remove duplicates
def remove_duplicates(df, df_name):
    print(f"Removing Duplicates for {df_name}")
    num_duplicates = df.duplicated().sum()
    if num_duplicates > 0:
        df.drop_duplicates(inplace=True)
        print(f"Removed {num_duplicates} duplicate rows from '{df_name}'.")

# Function to convert datetime columns
def convert_datetime(df, df_name):
    print(f"Converting Datetime Columns for {df_name}")
    if 'datetime' in df.columns:
        try:
            df['datetime'] = pd.to_datetime(df['datetime'])
            print(f"Converted 'datetime' column to datetime objects for {df_name}.")
        except ValueError as e:
            print(f"Error converting 'datetime' column in {df_name}: {e}")


# Iterate through each DataFrame and apply the functions
for df_name in ["df_errors", "df_failures", "df_machines", "df_maint", "df_telemetry"]:
    df = eval(df_name)
    handle_missing_values(df, df_name)
    address_outliers(df, df_name)
    remove_duplicates(df, df_name)
    convert_datetime(df, df_name)





# Merge telemetry data with errors, failures, and maintenance data
merged_df = pd.merge(df_telemetry, df_errors, on=['machineID', 'datetime'], how='left')
merged_df = pd.merge(merged_df, df_failures, on=['machineID', 'datetime'], how='left')
merged_df = pd.merge(merged_df, df_maint, on=['machineID', 'datetime'], how='left')

# Merge with machine features
merged_df = pd.merge(merged_df, df_machines, on='machineID', how='left')

# Calculate time since last maintenance
merged_df['datetime'] = pd.to_datetime(merged_df['datetime'])
merged_df = merged_df.sort_values(['machineID', 'datetime'])
merged_df['time_since_last_maint'] = merged_df.groupby('machineID')['datetime'].diff().dt.total_seconds()

# Calculate rolling averages for sensor readings
for col in ['volt', 'rotate', 'pressure', 'vibration']:
    merged_df[f'{col}_rolling_mean'] = merged_df.groupby('machineID')[col].rolling(window=10, min_periods=1).mean().reset_index(0, drop=True)

# Inspect the merged DataFrame
print(merged_df.shape)
print(merged_df.dtypes)
display(merged_df.describe(include='all'))
display(merged_df.head())





import pandas as pd
import numpy as np

# Calculate failure rate by machine model
failure_rate_by_model = merged_df.groupby('model')['failure'].count() / merged_df.groupby('model')['machineID'].count()
print("Failure Rate by Model:\n", failure_rate_by_model)

# Calculate failure rate by machine age group
merged_df['age_group'] = pd.cut(merged_df['age'], bins=[0, 5, 10, 15, 20, np.inf], labels=['0-5', '5-10', '10-15', '15-20', '20+'])
failure_rate_by_age = merged_df.groupby('age_group')['failure'].count() / merged_df.groupby('age_group')['machineID'].count()
print("\nFailure Rate by Age Group:\n", failure_rate_by_age)

# Analyze the distribution of failures over time
failures_over_time = merged_df.groupby('datetime')['failure'].count()
print("\nFailures Over Time:\n", failures_over_time.head())

# Analyze sensor readings before failures
# First, ensure that 'failure' is not null
merged_df_failures = merged_df[merged_df['failure'].notnull()]

# Calculate the mean sensor readings 10 time steps before a failure
time_steps_before_failure = 10
sensor_readings_before_failure = []
for index, row in merged_df_failures.iterrows():
    machine_id = row['machineID']
    failure_time = row['datetime']
    start_time = failure_time - pd.Timedelta(minutes=time_steps_before_failure)
    relevant_data = merged_df[(merged_df['machineID'] == machine_id) & (merged_df['datetime'] >= start_time) & (merged_df['datetime'] < failure_time)]
    if not relevant_data.empty:
        sensor_readings_before_failure.append(relevant_data[['volt', 'rotate', 'pressure', 'vibration']].mean())

# Convert the list to a dataframe for analysis
sensor_readings_before_failure = pd.DataFrame(sensor_readings_before_failure)

# Display the first few rows
print("\nSensor readings before failure:\n", sensor_readings_before_failure.head())

# Correlation analysis
correlation_matrix = merged_df[['volt', 'rotate', 'pressure', 'vibration', 'volt_rolling_mean', 'rotate_rolling_mean', 'pressure_rolling_mean', 'vibration_rolling_mean', 'failure']].corr()
print("\nCorrelation Matrix:\n", correlation_matrix)



import pandas as pd
import numpy as np

# Calculate failure rate by machine model
failure_rate_by_model = merged_df.groupby('model')['failure'].count() / merged_df.groupby('model')['machineID'].count()
print("Failure Rate by Model:\n", failure_rate_by_model)

# Calculate failure rate by machine age group
merged_df['age_group'] = pd.cut(merged_df['age'], bins=[0, 5, 10, 15, 20, np.inf], labels=['0-5', '5-10', '10-15', '15-20', '20+'])
failure_rate_by_age = merged_df.groupby('age_group')['failure'].count() / merged_df.groupby('age_group')['machineID'].count()
print("\nFailure Rate by Age Group:\n", failure_rate_by_age)

# Analyze the distribution of failures over time
failures_over_time = merged_df.groupby('datetime')['failure'].count()
print("\nFailures Over Time:\n", failures_over_time.head())

# Analyze sensor readings before failures
# First, ensure that 'failure' is not null and convert 'failure' to numeric representation
merged_df_failures = merged_df[merged_df['failure'].notnull()]
# Create a mapping for failures
failure_mapping = {failure: i for i, failure in enumerate(merged_df_failures['failure'].unique())}
merged_df_failures['failure_numeric'] = merged_df_failures['failure'].map(failure_mapping)

# Calculate the mean sensor readings 10 time steps before a failure
time_steps_before_failure = 10
sensor_readings_before_failure = []
for index, row in merged_df_failures.iterrows():
    machine_id = row['machineID']
    failure_time = row['datetime']
    start_time = failure_time - pd.Timedelta(minutes=time_steps_before_failure)
    relevant_data = merged_df[(merged_df['machineID'] == machine_id) & (merged_df['datetime'] >= start_time) & (merged_df['datetime'] < failure_time)]
    if not relevant_data.empty:
        sensor_readings_before_failure.append(relevant_data[['volt', 'rotate', 'pressure', 'vibration']].mean())

# Convert the list to a dataframe for analysis
sensor_readings_before_failure_df = pd.DataFrame(sensor_readings_before_failure)
print("\nSensor Readings Before Failure:\n", sensor_readings_before_failure_df.head())

# Correlation analysis - exclude non-numeric 'failure' column
correlation_matrix = merged_df[['volt', 'rotate', 'pressure', 'vibration', 'volt_rolling_mean', 'rotate_rolling_mean', 'pressure_rolling_mean', 'vibration_rolling_mean']].corr()
print("\nCorrelation Matrix:\n", correlation_matrix)





import matplotlib.pyplot as plt
import seaborn as sns

# 1. Failure Rate by Model
plt.figure(figsize=(10, 6))
failure_rate_by_model = merged_df.groupby('model')['failure'].count() / merged_df.groupby('model')['machineID'].count()
sns.barplot(x=failure_rate_by_model.index, y=failure_rate_by_model.values)
plt.xlabel("Machine Model")
plt.ylabel("Failure Rate")
plt.title("Failure Rate by Machine Model")
plt.show()

# 2. Failure Rate by Age Group
plt.figure(figsize=(10, 6))
failure_rate_by_age = merged_df.groupby('age_group')['failure'].count() / merged_df.groupby('age_group')['machineID'].count()
sns.barplot(x=failure_rate_by_age.index, y=failure_rate_by_age.values)
plt.xlabel("Machine Age Group")
plt.ylabel("Failure Rate")
plt.title("Failure Rate by Machine Age Group")
plt.show()

# 3. Failures Over Time
plt.figure(figsize=(12, 6))
failures_over_time = merged_df.groupby('datetime')['failure'].count()
plt.plot(failures_over_time.index, failures_over_time.values)
plt.xlabel("Date and Time")
plt.ylabel("Number of Failures")
plt.title("Number of Failures Over Time")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# 4. Sensor Readings Over Time (for a sample of machines)
machine_ids = merged_df['machineID'].unique()[:5]  # Select the first 5 machines
plt.figure(figsize=(15, 10))

for i, machine_id in enumerate(machine_ids):
  plt.subplot(len(machine_ids), 1, i+1)
  machine_data = merged_df[merged_df["machineID"] == machine_id]
  plt.plot(machine_data['datetime'], machine_data['volt'], label='Volt')
  plt.plot(machine_data['datetime'], machine_data['rotate'], label='Rotate')
  plt.plot(machine_data['datetime'], machine_data['pressure'], label='Pressure')
  plt.plot(machine_data['datetime'], machine_data['vibration'], label='Vibration')
  plt.xlabel('Datetime')
  plt.ylabel('Sensor Readings')
  plt.title(f'Sensor Readings for Machine {machine_id}')
  plt.legend()
  plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# 5. Correlation Heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = merged_df[['volt', 'rotate', 'pressure', 'vibration', 'volt_rolling_mean', 'rotate_rolling_mean', 'pressure_rolling_mean', 'vibration_rolling_mean']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Sensor Readings and Rolling Means")
plt.show()





import matplotlib.pyplot as plt

# Time Series Analysis for df_telemetry
for machine_id in df_telemetry['machineID'].unique():
    machine_data = df_telemetry[df_telemetry['machineID'] == machine_id]

    # Convert 'datetime' to datetime objects
    machine_data['datetime'] = pd.to_datetime(machine_data['datetime'])

    # Set 'datetime' as index
    machine_data = machine_data.set_index('datetime')

    plt.figure(figsize=(12, 8))

    # Plot each telemetry feature
    for column in ['volt', 'rotate', 'pressure', 'vibration']:
        plt.plot(machine_data.index, machine_data[column], label=column)

        # Calculate and plot rolling mean (e.g., window of 24 hours)
        rolling_mean = machine_data[column].rolling(window=24).mean()
        plt.plot(machine_data.index, rolling_mean, label=f"{column} rolling mean (24h)", linestyle='--')


    plt.title(f"Telemetry Data for Machine {machine_id}")
    plt.xlabel("Datetime")
    plt.ylabel("Value")
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()





# prompt: show features in the same plot

# Time Series Analysis for df_telemetry using Plotly - All features on the same plot
for machine_id in df_telemetry['machineID'].unique()[:3]: # Only plot for the first 3 machines
    machine_data = df_telemetry[df_telemetry['machineID'] == machine_id].copy()
    machine_data.loc[:, 'datetime'] = pd.to_datetime(machine_data['datetime'])

    # Set 'datetime' as index
    machine_data = machine_data.set_index('datetime')

    # Create a single figure for all features
    fig = go.Figure()

    features = ['volt', 'rotate', 'pressure', 'vibration']
    colors = ['blue', 'red', 'green', 'purple'] # Define colors for each feature

    for i, column in enumerate(features):
        # Add trace for the original data
        fig.add_trace(go.Scattergl(x=machine_data.index, y=machine_data[column], mode='lines', name=column, line=dict(color=colors[i])))

        # Calculate and add trace for rolling mean (e.g., window of 24 hours)
        rolling_mean = machine_data[column].rolling(window=24).mean()
        fig.add_trace(go.Scattergl(x=machine_data.index, y=rolling_mean, mode='lines', name=f"{column} rolling mean (24h)", line=dict(dash='dash', color=colors[i])))

    # Update layout
    fig.update_layout(
        title_text=f"Telemetry Data for Machine {machine_id}",
        height=600,
        xaxis_title="Datetime",
        yaxis_title="Value",
        hovermode='x unified' # Unify hover for better comparison
    )

    # Update x-axis to show datetime with hour and add range slider
    fig.update_xaxes(
        tickformat="%y:%m:%d | %H:%M",
        rangeslider_visible=True
    )

    fig.show()



